{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LFopIDwcj4eR"},"source":["# Libraries for Neural Networks - sklearn 1 (classification)\n","\n","\n","![alt text](https://drive.google.com/uc?id=1xgZhek0467AtlfupqvovcjoFIJ2dB4in)\n"]},{"cell_type":"markdown","metadata":{"id":"hmwQCDnBmJil"},"source":["## Loading the data"]},{"cell_type":"code","source":["# MLPClassifier : multi layer classifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import datasets"],"metadata":{"id":"HHUham-j5a5v","executionInfo":{"status":"ok","timestamp":1670732727724,"user_tz":-540,"elapsed":1102,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["iris = datasets.load_iris()\n","inputs = iris.data\n","outputs = iris.target"],"metadata":{"id":"fcaTRw9_5jEd","executionInfo":{"status":"ok","timestamp":1670732727724,"user_tz":-540,"elapsed":2,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UF29OkpYmMsV"},"source":["## Train and test datasets"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, random_state=0)"],"metadata":{"id":"LL-BAvu16NgY","executionInfo":{"status":"ok","timestamp":1670732727724,"user_tz":-540,"elapsed":2,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4S5gJUwomQ3U"},"source":["## Neural network (training)"]},{"cell_type":"code","source":["# MLPClassifier(max_iter:epochs num, verbose:print process)\n","# If you see this error message, change tol value as message say.\n","# \"Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n","# MLPClassifier(max_iter=1000, tol=1e-05, verbose=True)\"\n","network = MLPClassifier(max_iter=2000, \n","                        verbose=True, \n","                        tol=1e-05, \n","                        activation='logistic',\n","                        solver='adam',\n","                        learning_rate = 'constant',\n","                        learning_rate_init=0.001,\n","                        batch_size=32,\n","                        hidden_layer_sizes=(4, 4))\n","network.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmT9lfxb7t1M","executionInfo":{"status":"ok","timestamp":1670732736846,"user_tz":-540,"elapsed":9124,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"e52753d4-3b17-407b-cd2b-67db32bd9840"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 1.21413537\n","Iteration 2, loss = 1.20910241\n","Iteration 3, loss = 1.20410858\n","Iteration 4, loss = 1.19929446\n","Iteration 5, loss = 1.19474276\n","Iteration 6, loss = 1.19057470\n","Iteration 7, loss = 1.18560234\n","Iteration 8, loss = 1.18145193\n","Iteration 9, loss = 1.17758824\n","Iteration 10, loss = 1.17375299\n","Iteration 11, loss = 1.16977789\n","Iteration 12, loss = 1.16603260\n","Iteration 13, loss = 1.16288596\n","Iteration 14, loss = 1.15926161\n","Iteration 15, loss = 1.15565163\n","Iteration 16, loss = 1.15231603\n","Iteration 17, loss = 1.14983216\n","Iteration 18, loss = 1.14667715\n","Iteration 19, loss = 1.14384719\n","Iteration 20, loss = 1.14056567\n","Iteration 21, loss = 1.13823057\n","Iteration 22, loss = 1.13580439\n","Iteration 23, loss = 1.13326973\n","Iteration 24, loss = 1.13061270\n","Iteration 25, loss = 1.12812414\n","Iteration 26, loss = 1.12600397\n","Iteration 27, loss = 1.12370616\n","Iteration 28, loss = 1.12183028\n","Iteration 29, loss = 1.11952337\n","Iteration 30, loss = 1.11749630\n","Iteration 31, loss = 1.11567561\n","Iteration 32, loss = 1.11366140\n","Iteration 33, loss = 1.11169830\n","Iteration 34, loss = 1.11014489\n","Iteration 35, loss = 1.10820520\n","Iteration 36, loss = 1.10664947\n","Iteration 37, loss = 1.10491933\n","Iteration 38, loss = 1.10362668\n","Iteration 39, loss = 1.10185805\n","Iteration 40, loss = 1.10035139\n","Iteration 41, loss = 1.09878934\n","Iteration 42, loss = 1.09746290\n","Iteration 43, loss = 1.09618781\n","Iteration 44, loss = 1.09462716\n","Iteration 45, loss = 1.09321822\n","Iteration 46, loss = 1.09199910\n","Iteration 47, loss = 1.09046105\n","Iteration 48, loss = 1.08930181\n","Iteration 49, loss = 1.08812717\n","Iteration 50, loss = 1.08677054\n","Iteration 51, loss = 1.08549738\n","Iteration 52, loss = 1.08421553\n","Iteration 53, loss = 1.08306267\n","Iteration 54, loss = 1.08174704\n","Iteration 55, loss = 1.08062367\n","Iteration 56, loss = 1.07937298\n","Iteration 57, loss = 1.07816147\n","Iteration 58, loss = 1.07698984\n","Iteration 59, loss = 1.07555671\n","Iteration 60, loss = 1.07453455\n","Iteration 61, loss = 1.07313839\n","Iteration 62, loss = 1.07208393\n","Iteration 63, loss = 1.07070929\n","Iteration 64, loss = 1.06960109\n","Iteration 65, loss = 1.06825522\n","Iteration 66, loss = 1.06695067\n","Iteration 67, loss = 1.06558586\n","Iteration 68, loss = 1.06427227\n","Iteration 69, loss = 1.06303605\n","Iteration 70, loss = 1.06165983\n","Iteration 71, loss = 1.06046024\n","Iteration 72, loss = 1.05911870\n","Iteration 73, loss = 1.05772030\n","Iteration 74, loss = 1.05633278\n","Iteration 75, loss = 1.05495085\n","Iteration 76, loss = 1.05356383\n","Iteration 77, loss = 1.05215980\n","Iteration 78, loss = 1.05063826\n","Iteration 79, loss = 1.04919632\n","Iteration 80, loss = 1.04772083\n","Iteration 81, loss = 1.04612492\n","Iteration 82, loss = 1.04459420\n","Iteration 83, loss = 1.04292764\n","Iteration 84, loss = 1.04142961\n","Iteration 85, loss = 1.03978872\n","Iteration 86, loss = 1.03802520\n","Iteration 87, loss = 1.03633386\n","Iteration 88, loss = 1.03457950\n","Iteration 89, loss = 1.03282844\n","Iteration 90, loss = 1.03098251\n","Iteration 91, loss = 1.02921535\n","Iteration 92, loss = 1.02728523\n","Iteration 93, loss = 1.02545373\n","Iteration 94, loss = 1.02369561\n","Iteration 95, loss = 1.02145257\n","Iteration 96, loss = 1.01944861\n","Iteration 97, loss = 1.01743547\n","Iteration 98, loss = 1.01541475\n","Iteration 99, loss = 1.01319551\n","Iteration 100, loss = 1.01106469\n","Iteration 101, loss = 1.00890765\n","Iteration 102, loss = 1.00659906\n","Iteration 103, loss = 1.00450236\n","Iteration 104, loss = 1.00210827\n","Iteration 105, loss = 0.99960309\n","Iteration 106, loss = 0.99725853\n","Iteration 107, loss = 0.99481390\n","Iteration 108, loss = 0.99238358\n","Iteration 109, loss = 0.98985376\n","Iteration 110, loss = 0.98732347\n","Iteration 111, loss = 0.98473993\n","Iteration 112, loss = 0.98207465\n","Iteration 113, loss = 0.97948321\n","Iteration 114, loss = 0.97700818\n","Iteration 115, loss = 0.97399245\n","Iteration 116, loss = 0.97122252\n","Iteration 117, loss = 0.96848793\n","Iteration 118, loss = 0.96565264\n","Iteration 119, loss = 0.96279797\n","Iteration 120, loss = 0.95988249\n","Iteration 121, loss = 0.95700671\n","Iteration 122, loss = 0.95396977\n","Iteration 123, loss = 0.95095898\n","Iteration 124, loss = 0.94793827\n","Iteration 125, loss = 0.94478164\n","Iteration 126, loss = 0.94163578\n","Iteration 127, loss = 0.93853183\n","Iteration 128, loss = 0.93538531\n","Iteration 129, loss = 0.93204832\n","Iteration 130, loss = 0.92881382\n","Iteration 131, loss = 0.92546922\n","Iteration 132, loss = 0.92227379\n","Iteration 133, loss = 0.91893382\n","Iteration 134, loss = 0.91542082\n","Iteration 135, loss = 0.91212454\n","Iteration 136, loss = 0.90848205\n","Iteration 137, loss = 0.90508121\n","Iteration 138, loss = 0.90161770\n","Iteration 139, loss = 0.89808808\n","Iteration 140, loss = 0.89453164\n","Iteration 141, loss = 0.89096720\n","Iteration 142, loss = 0.88736628\n","Iteration 143, loss = 0.88380454\n","Iteration 144, loss = 0.88011724\n","Iteration 145, loss = 0.87645203\n","Iteration 146, loss = 0.87279503\n","Iteration 147, loss = 0.86903907\n","Iteration 148, loss = 0.86526344\n","Iteration 149, loss = 0.86152844\n","Iteration 150, loss = 0.85788812\n","Iteration 151, loss = 0.85393451\n","Iteration 152, loss = 0.85019282\n","Iteration 153, loss = 0.84626627\n","Iteration 154, loss = 0.84244129\n","Iteration 155, loss = 0.83863776\n","Iteration 156, loss = 0.83492449\n","Iteration 157, loss = 0.83092455\n","Iteration 158, loss = 0.82721596\n","Iteration 159, loss = 0.82327972\n","Iteration 160, loss = 0.81949812\n","Iteration 161, loss = 0.81568539\n","Iteration 162, loss = 0.81196334\n","Iteration 163, loss = 0.80813425\n","Iteration 164, loss = 0.80435682\n","Iteration 165, loss = 0.80073212\n","Iteration 166, loss = 0.79676757\n","Iteration 167, loss = 0.79309928\n","Iteration 168, loss = 0.78944660\n","Iteration 169, loss = 0.78577270\n","Iteration 170, loss = 0.78208032\n","Iteration 171, loss = 0.77836879\n","Iteration 172, loss = 0.77472641\n","Iteration 173, loss = 0.77115226\n","Iteration 174, loss = 0.76760686\n","Iteration 175, loss = 0.76402423\n","Iteration 176, loss = 0.76047087\n","Iteration 177, loss = 0.75703051\n","Iteration 178, loss = 0.75347275\n","Iteration 179, loss = 0.75008327\n","Iteration 180, loss = 0.74659998\n","Iteration 181, loss = 0.74323403\n","Iteration 182, loss = 0.73992874\n","Iteration 183, loss = 0.73652937\n","Iteration 184, loss = 0.73324353\n","Iteration 185, loss = 0.72996249\n","Iteration 186, loss = 0.72682029\n","Iteration 187, loss = 0.72360922\n","Iteration 188, loss = 0.72045616\n","Iteration 189, loss = 0.71727069\n","Iteration 190, loss = 0.71423924\n","Iteration 191, loss = 0.71120041\n","Iteration 192, loss = 0.70817251\n","Iteration 193, loss = 0.70519113\n","Iteration 194, loss = 0.70232887\n","Iteration 195, loss = 0.69940828\n","Iteration 196, loss = 0.69650611\n","Iteration 197, loss = 0.69382946\n","Iteration 198, loss = 0.69090723\n","Iteration 199, loss = 0.68821239\n","Iteration 200, loss = 0.68549474\n","Iteration 201, loss = 0.68289654\n","Iteration 202, loss = 0.68021783\n","Iteration 203, loss = 0.67771125\n","Iteration 204, loss = 0.67504483\n","Iteration 205, loss = 0.67244772\n","Iteration 206, loss = 0.66999279\n","Iteration 207, loss = 0.66750149\n","Iteration 208, loss = 0.66507224\n","Iteration 209, loss = 0.66272443\n","Iteration 210, loss = 0.66038988\n","Iteration 211, loss = 0.65802435\n","Iteration 212, loss = 0.65573195\n","Iteration 213, loss = 0.65351048\n","Iteration 214, loss = 0.65118569\n","Iteration 215, loss = 0.64908720\n","Iteration 216, loss = 0.64681077\n","Iteration 217, loss = 0.64475344\n","Iteration 218, loss = 0.64263396\n","Iteration 219, loss = 0.64049406\n","Iteration 220, loss = 0.63854710\n","Iteration 221, loss = 0.63649132\n","Iteration 222, loss = 0.63453700\n","Iteration 223, loss = 0.63254014\n","Iteration 224, loss = 0.63064349\n","Iteration 225, loss = 0.62874601\n","Iteration 226, loss = 0.62681689\n","Iteration 227, loss = 0.62510290\n","Iteration 228, loss = 0.62321584\n","Iteration 229, loss = 0.62151181\n","Iteration 230, loss = 0.61968870\n","Iteration 231, loss = 0.61791239\n","Iteration 232, loss = 0.61628818\n","Iteration 233, loss = 0.61456071\n","Iteration 234, loss = 0.61289677\n","Iteration 235, loss = 0.61132016\n","Iteration 236, loss = 0.60965351\n","Iteration 237, loss = 0.60808051\n","Iteration 238, loss = 0.60654108\n","Iteration 239, loss = 0.60498160\n","Iteration 240, loss = 0.60349769\n","Iteration 241, loss = 0.60197822\n","Iteration 242, loss = 0.60048883\n","Iteration 243, loss = 0.59904205\n","Iteration 244, loss = 0.59762523\n","Iteration 245, loss = 0.59625664\n","Iteration 246, loss = 0.59484079\n","Iteration 247, loss = 0.59349636\n","Iteration 248, loss = 0.59213763\n","Iteration 249, loss = 0.59082457\n","Iteration 250, loss = 0.58950831\n","Iteration 251, loss = 0.58819736\n","Iteration 252, loss = 0.58695844\n","Iteration 253, loss = 0.58568405\n","Iteration 254, loss = 0.58449171\n","Iteration 255, loss = 0.58324154\n","Iteration 256, loss = 0.58206491\n","Iteration 257, loss = 0.58085272\n","Iteration 258, loss = 0.57974242\n","Iteration 259, loss = 0.57856639\n","Iteration 260, loss = 0.57743591\n","Iteration 261, loss = 0.57636349\n","Iteration 262, loss = 0.57521047\n","Iteration 263, loss = 0.57411823\n","Iteration 264, loss = 0.57309749\n","Iteration 265, loss = 0.57201905\n","Iteration 266, loss = 0.57100687\n","Iteration 267, loss = 0.56996684\n","Iteration 268, loss = 0.56898150\n","Iteration 269, loss = 0.56794810\n","Iteration 270, loss = 0.56700849\n","Iteration 271, loss = 0.56602193\n","Iteration 272, loss = 0.56504428\n","Iteration 273, loss = 0.56412681\n","Iteration 274, loss = 0.56315532\n","Iteration 275, loss = 0.56228561\n","Iteration 276, loss = 0.56134756\n","Iteration 277, loss = 0.56047958\n","Iteration 278, loss = 0.55957421\n","Iteration 279, loss = 0.55869897\n","Iteration 280, loss = 0.55783577\n","Iteration 281, loss = 0.55702960\n","Iteration 282, loss = 0.55615359\n","Iteration 283, loss = 0.55537182\n","Iteration 284, loss = 0.55452003\n","Iteration 285, loss = 0.55372519\n","Iteration 286, loss = 0.55294555\n","Iteration 287, loss = 0.55216095\n","Iteration 288, loss = 0.55139594\n","Iteration 289, loss = 0.55062590\n","Iteration 290, loss = 0.54990532\n","Iteration 291, loss = 0.54910110\n","Iteration 292, loss = 0.54839203\n","Iteration 293, loss = 0.54765678\n","Iteration 294, loss = 0.54694089\n","Iteration 295, loss = 0.54624359\n","Iteration 296, loss = 0.54554297\n","Iteration 297, loss = 0.54488619\n","Iteration 298, loss = 0.54418643\n","Iteration 299, loss = 0.54348037\n","Iteration 300, loss = 0.54284710\n","Iteration 301, loss = 0.54217939\n","Iteration 302, loss = 0.54153623\n","Iteration 303, loss = 0.54089539\n","Iteration 304, loss = 0.54026274\n","Iteration 305, loss = 0.53962282\n","Iteration 306, loss = 0.53899313\n","Iteration 307, loss = 0.53837691\n","Iteration 308, loss = 0.53776119\n","Iteration 309, loss = 0.53717799\n","Iteration 310, loss = 0.53661783\n","Iteration 311, loss = 0.53601476\n","Iteration 312, loss = 0.53548043\n","Iteration 313, loss = 0.53484209\n","Iteration 314, loss = 0.53432292\n","Iteration 315, loss = 0.53377520\n","Iteration 316, loss = 0.53318305\n","Iteration 317, loss = 0.53262461\n","Iteration 318, loss = 0.53209811\n","Iteration 319, loss = 0.53161221\n","Iteration 320, loss = 0.53106274\n","Iteration 321, loss = 0.53051558\n","Iteration 322, loss = 0.52999921\n","Iteration 323, loss = 0.52947351\n","Iteration 324, loss = 0.52897560\n","Iteration 325, loss = 0.52848885\n","Iteration 326, loss = 0.52799444\n","Iteration 327, loss = 0.52750033\n","Iteration 328, loss = 0.52701587\n","Iteration 329, loss = 0.52652623\n","Iteration 330, loss = 0.52615219\n","Iteration 331, loss = 0.52564711\n","Iteration 332, loss = 0.52512396\n","Iteration 333, loss = 0.52469617\n","Iteration 334, loss = 0.52422550\n","Iteration 335, loss = 0.52379184\n","Iteration 336, loss = 0.52331348\n","Iteration 337, loss = 0.52287814\n","Iteration 338, loss = 0.52244820\n","Iteration 339, loss = 0.52200788\n","Iteration 340, loss = 0.52157495\n","Iteration 341, loss = 0.52115054\n","Iteration 342, loss = 0.52072863\n","Iteration 343, loss = 0.52032430\n","Iteration 344, loss = 0.51989667\n","Iteration 345, loss = 0.51949577\n","Iteration 346, loss = 0.51910278\n","Iteration 347, loss = 0.51869559\n","Iteration 348, loss = 0.51834638\n","Iteration 349, loss = 0.51790124\n","Iteration 350, loss = 0.51751753\n","Iteration 351, loss = 0.51714194\n","Iteration 352, loss = 0.51674749\n","Iteration 353, loss = 0.51638226\n","Iteration 354, loss = 0.51599351\n","Iteration 355, loss = 0.51561277\n","Iteration 356, loss = 0.51526635\n","Iteration 357, loss = 0.51491794\n","Iteration 358, loss = 0.51458389\n","Iteration 359, loss = 0.51416930\n","Iteration 360, loss = 0.51381061\n","Iteration 361, loss = 0.51345941\n","Iteration 362, loss = 0.51309876\n","Iteration 363, loss = 0.51276845\n","Iteration 364, loss = 0.51242822\n","Iteration 365, loss = 0.51207784\n","Iteration 366, loss = 0.51172782\n","Iteration 367, loss = 0.51145149\n","Iteration 368, loss = 0.51109403\n","Iteration 369, loss = 0.51073417\n","Iteration 370, loss = 0.51047667\n","Iteration 371, loss = 0.51008882\n","Iteration 372, loss = 0.50982766\n","Iteration 373, loss = 0.50944250\n","Iteration 374, loss = 0.50915138\n","Iteration 375, loss = 0.50881116\n","Iteration 376, loss = 0.50848720\n","Iteration 377, loss = 0.50817621\n","Iteration 378, loss = 0.50786717\n","Iteration 379, loss = 0.50759271\n","Iteration 380, loss = 0.50725897\n","Iteration 381, loss = 0.50696381\n","Iteration 382, loss = 0.50665564\n","Iteration 383, loss = 0.50636821\n","Iteration 384, loss = 0.50607541\n","Iteration 385, loss = 0.50576666\n","Iteration 386, loss = 0.50547568\n","Iteration 387, loss = 0.50519405\n","Iteration 388, loss = 0.50491193\n","Iteration 389, loss = 0.50460958\n","Iteration 390, loss = 0.50431618\n","Iteration 391, loss = 0.50403989\n","Iteration 392, loss = 0.50377190\n","Iteration 393, loss = 0.50346138\n","Iteration 394, loss = 0.50318206\n","Iteration 395, loss = 0.50292582\n","Iteration 396, loss = 0.50263463\n","Iteration 397, loss = 0.50239200\n","Iteration 398, loss = 0.50207975\n","Iteration 399, loss = 0.50179973\n","Iteration 400, loss = 0.50153059\n","Iteration 401, loss = 0.50131132\n","Iteration 402, loss = 0.50099935\n","Iteration 403, loss = 0.50071897\n","Iteration 404, loss = 0.50048433\n","Iteration 405, loss = 0.50020998\n","Iteration 406, loss = 0.49993160\n","Iteration 407, loss = 0.49964952\n","Iteration 408, loss = 0.49938976\n","Iteration 409, loss = 0.49916968\n","Iteration 410, loss = 0.49892541\n","Iteration 411, loss = 0.49863845\n","Iteration 412, loss = 0.49835826\n","Iteration 413, loss = 0.49810526\n","Iteration 414, loss = 0.49783072\n","Iteration 415, loss = 0.49757910\n","Iteration 416, loss = 0.49731951\n","Iteration 417, loss = 0.49705801\n","Iteration 418, loss = 0.49681068\n","Iteration 419, loss = 0.49653409\n","Iteration 420, loss = 0.49628211\n","Iteration 421, loss = 0.49602810\n","Iteration 422, loss = 0.49576894\n","Iteration 423, loss = 0.49552143\n","Iteration 424, loss = 0.49526118\n","Iteration 425, loss = 0.49508986\n","Iteration 426, loss = 0.49481115\n","Iteration 427, loss = 0.49448549\n","Iteration 428, loss = 0.49428311\n","Iteration 429, loss = 0.49398048\n","Iteration 430, loss = 0.49371915\n","Iteration 431, loss = 0.49348438\n","Iteration 432, loss = 0.49320681\n","Iteration 433, loss = 0.49297512\n","Iteration 434, loss = 0.49269847\n","Iteration 435, loss = 0.49243211\n","Iteration 436, loss = 0.49217852\n","Iteration 437, loss = 0.49192254\n","Iteration 438, loss = 0.49165674\n","Iteration 439, loss = 0.49139008\n","Iteration 440, loss = 0.49119277\n","Iteration 441, loss = 0.49089033\n","Iteration 442, loss = 0.49062008\n","Iteration 443, loss = 0.49034801\n","Iteration 444, loss = 0.49015563\n","Iteration 445, loss = 0.48988383\n","Iteration 446, loss = 0.48958448\n","Iteration 447, loss = 0.48932827\n","Iteration 448, loss = 0.48902254\n","Iteration 449, loss = 0.48879785\n","Iteration 450, loss = 0.48852947\n","Iteration 451, loss = 0.48823329\n","Iteration 452, loss = 0.48797294\n","Iteration 453, loss = 0.48769757\n","Iteration 454, loss = 0.48740056\n","Iteration 455, loss = 0.48714690\n","Iteration 456, loss = 0.48688197\n","Iteration 457, loss = 0.48657769\n","Iteration 458, loss = 0.48629372\n","Iteration 459, loss = 0.48604606\n","Iteration 460, loss = 0.48573050\n","Iteration 461, loss = 0.48551898\n","Iteration 462, loss = 0.48516570\n","Iteration 463, loss = 0.48494063\n","Iteration 464, loss = 0.48461357\n","Iteration 465, loss = 0.48428298\n","Iteration 466, loss = 0.48402099\n","Iteration 467, loss = 0.48377549\n","Iteration 468, loss = 0.48340131\n","Iteration 469, loss = 0.48309732\n","Iteration 470, loss = 0.48279181\n","Iteration 471, loss = 0.48250059\n","Iteration 472, loss = 0.48223352\n","Iteration 473, loss = 0.48188160\n","Iteration 474, loss = 0.48158985\n","Iteration 475, loss = 0.48139120\n","Iteration 476, loss = 0.48092366\n","Iteration 477, loss = 0.48062969\n","Iteration 478, loss = 0.48031924\n","Iteration 479, loss = 0.47999480\n","Iteration 480, loss = 0.47969237\n","Iteration 481, loss = 0.47934173\n","Iteration 482, loss = 0.47901941\n","Iteration 483, loss = 0.47868585\n","Iteration 484, loss = 0.47838733\n","Iteration 485, loss = 0.47801595\n","Iteration 486, loss = 0.47764776\n","Iteration 487, loss = 0.47727934\n","Iteration 488, loss = 0.47692330\n","Iteration 489, loss = 0.47659971\n","Iteration 490, loss = 0.47622175\n","Iteration 491, loss = 0.47585471\n","Iteration 492, loss = 0.47548505\n","Iteration 493, loss = 0.47512896\n","Iteration 494, loss = 0.47477833\n","Iteration 495, loss = 0.47436050\n","Iteration 496, loss = 0.47400573\n","Iteration 497, loss = 0.47361749\n","Iteration 498, loss = 0.47322388\n","Iteration 499, loss = 0.47283046\n","Iteration 500, loss = 0.47243472\n","Iteration 501, loss = 0.47208660\n","Iteration 502, loss = 0.47158918\n","Iteration 503, loss = 0.47134995\n","Iteration 504, loss = 0.47082820\n","Iteration 505, loss = 0.47039851\n","Iteration 506, loss = 0.46994634\n","Iteration 507, loss = 0.46953193\n","Iteration 508, loss = 0.46913106\n","Iteration 509, loss = 0.46860881\n","Iteration 510, loss = 0.46822564\n","Iteration 511, loss = 0.46773278\n","Iteration 512, loss = 0.46724858\n","Iteration 513, loss = 0.46677660\n","Iteration 514, loss = 0.46630139\n","Iteration 515, loss = 0.46583298\n","Iteration 516, loss = 0.46531944\n","Iteration 517, loss = 0.46482836\n","Iteration 518, loss = 0.46437208\n","Iteration 519, loss = 0.46375092\n","Iteration 520, loss = 0.46320570\n","Iteration 521, loss = 0.46270815\n","Iteration 522, loss = 0.46211998\n","Iteration 523, loss = 0.46157784\n","Iteration 524, loss = 0.46104290\n","Iteration 525, loss = 0.46048468\n","Iteration 526, loss = 0.45984041\n","Iteration 527, loss = 0.45930673\n","Iteration 528, loss = 0.45874203\n","Iteration 529, loss = 0.45812787\n","Iteration 530, loss = 0.45745438\n","Iteration 531, loss = 0.45679219\n","Iteration 532, loss = 0.45622188\n","Iteration 533, loss = 0.45578407\n","Iteration 534, loss = 0.45492361\n","Iteration 535, loss = 0.45434065\n","Iteration 536, loss = 0.45361937\n","Iteration 537, loss = 0.45295987\n","Iteration 538, loss = 0.45234404\n","Iteration 539, loss = 0.45160550\n","Iteration 540, loss = 0.45096103\n","Iteration 541, loss = 0.45033468\n","Iteration 542, loss = 0.44958135\n","Iteration 543, loss = 0.44900359\n","Iteration 544, loss = 0.44834559\n","Iteration 545, loss = 0.44750899\n","Iteration 546, loss = 0.44685276\n","Iteration 547, loss = 0.44618528\n","Iteration 548, loss = 0.44551053\n","Iteration 549, loss = 0.44480931\n","Iteration 550, loss = 0.44399109\n","Iteration 551, loss = 0.44329363\n","Iteration 552, loss = 0.44253106\n","Iteration 553, loss = 0.44181217\n","Iteration 554, loss = 0.44100039\n","Iteration 555, loss = 0.44028450\n","Iteration 556, loss = 0.43955678\n","Iteration 557, loss = 0.43882725\n","Iteration 558, loss = 0.43805484\n","Iteration 559, loss = 0.43726579\n","Iteration 560, loss = 0.43645096\n","Iteration 561, loss = 0.43573766\n","Iteration 562, loss = 0.43501654\n","Iteration 563, loss = 0.43409776\n","Iteration 564, loss = 0.43323575\n","Iteration 565, loss = 0.43243922\n","Iteration 566, loss = 0.43172722\n","Iteration 567, loss = 0.43080051\n","Iteration 568, loss = 0.43008785\n","Iteration 569, loss = 0.42922900\n","Iteration 570, loss = 0.42828627\n","Iteration 571, loss = 0.42737624\n","Iteration 572, loss = 0.42646676\n","Iteration 573, loss = 0.42566108\n","Iteration 574, loss = 0.42479120\n","Iteration 575, loss = 0.42392047\n","Iteration 576, loss = 0.42300188\n","Iteration 577, loss = 0.42216198\n","Iteration 578, loss = 0.42135024\n","Iteration 579, loss = 0.42027447\n","Iteration 580, loss = 0.41934555\n","Iteration 581, loss = 0.41845954\n","Iteration 582, loss = 0.41770910\n","Iteration 583, loss = 0.41682779\n","Iteration 584, loss = 0.41576990\n","Iteration 585, loss = 0.41487559\n","Iteration 586, loss = 0.41379829\n","Iteration 587, loss = 0.41284730\n","Iteration 588, loss = 0.41190581\n","Iteration 589, loss = 0.41114144\n","Iteration 590, loss = 0.41000818\n","Iteration 591, loss = 0.40910581\n","Iteration 592, loss = 0.40810223\n","Iteration 593, loss = 0.40707777\n","Iteration 594, loss = 0.40615520\n","Iteration 595, loss = 0.40520250\n","Iteration 596, loss = 0.40432335\n","Iteration 597, loss = 0.40320184\n","Iteration 598, loss = 0.40223637\n","Iteration 599, loss = 0.40131998\n","Iteration 600, loss = 0.40033512\n","Iteration 601, loss = 0.39940732\n","Iteration 602, loss = 0.39857663\n","Iteration 603, loss = 0.39740092\n","Iteration 604, loss = 0.39642068\n","Iteration 605, loss = 0.39544011\n","Iteration 606, loss = 0.39473533\n","Iteration 607, loss = 0.39352254\n","Iteration 608, loss = 0.39255848\n","Iteration 609, loss = 0.39147622\n","Iteration 610, loss = 0.39047402\n","Iteration 611, loss = 0.38968288\n","Iteration 612, loss = 0.38862039\n","Iteration 613, loss = 0.38759242\n","Iteration 614, loss = 0.38669643\n","Iteration 615, loss = 0.38576662\n","Iteration 616, loss = 0.38470489\n","Iteration 617, loss = 0.38369695\n","Iteration 618, loss = 0.38290308\n","Iteration 619, loss = 0.38178683\n","Iteration 620, loss = 0.38075826\n","Iteration 621, loss = 0.37971472\n","Iteration 622, loss = 0.37882989\n","Iteration 623, loss = 0.37780142\n","Iteration 624, loss = 0.37674791\n","Iteration 625, loss = 0.37586447\n","Iteration 626, loss = 0.37479505\n","Iteration 627, loss = 0.37387293\n","Iteration 628, loss = 0.37286749\n","Iteration 629, loss = 0.37185179\n","Iteration 630, loss = 0.37090068\n","Iteration 631, loss = 0.36991892\n","Iteration 632, loss = 0.36895601\n","Iteration 633, loss = 0.36793815\n","Iteration 634, loss = 0.36690475\n","Iteration 635, loss = 0.36595941\n","Iteration 636, loss = 0.36515882\n","Iteration 637, loss = 0.36405747\n","Iteration 638, loss = 0.36324472\n","Iteration 639, loss = 0.36207093\n","Iteration 640, loss = 0.36111937\n","Iteration 641, loss = 0.36016883\n","Iteration 642, loss = 0.35919441\n","Iteration 643, loss = 0.35831319\n","Iteration 644, loss = 0.35733530\n","Iteration 645, loss = 0.35629738\n","Iteration 646, loss = 0.35536743\n","Iteration 647, loss = 0.35434920\n","Iteration 648, loss = 0.35351184\n","Iteration 649, loss = 0.35242908\n","Iteration 650, loss = 0.35161793\n","Iteration 651, loss = 0.35049669\n","Iteration 652, loss = 0.34970481\n","Iteration 653, loss = 0.34862110\n","Iteration 654, loss = 0.34777290\n","Iteration 655, loss = 0.34679500\n","Iteration 656, loss = 0.34580761\n","Iteration 657, loss = 0.34477184\n","Iteration 658, loss = 0.34393050\n","Iteration 659, loss = 0.34315156\n","Iteration 660, loss = 0.34200307\n","Iteration 661, loss = 0.34112053\n","Iteration 662, loss = 0.34025411\n","Iteration 663, loss = 0.33926103\n","Iteration 664, loss = 0.33821240\n","Iteration 665, loss = 0.33735753\n","Iteration 666, loss = 0.33640007\n","Iteration 667, loss = 0.33548120\n","Iteration 668, loss = 0.33462503\n","Iteration 669, loss = 0.33364312\n","Iteration 670, loss = 0.33277663\n","Iteration 671, loss = 0.33191293\n","Iteration 672, loss = 0.33093041\n","Iteration 673, loss = 0.32999913\n","Iteration 674, loss = 0.32904699\n","Iteration 675, loss = 0.32842752\n","Iteration 676, loss = 0.32756559\n","Iteration 677, loss = 0.32635125\n","Iteration 678, loss = 0.32539842\n","Iteration 679, loss = 0.32461811\n","Iteration 680, loss = 0.32363043\n","Iteration 681, loss = 0.32277373\n","Iteration 682, loss = 0.32188501\n","Iteration 683, loss = 0.32105983\n","Iteration 684, loss = 0.32025904\n","Iteration 685, loss = 0.31922737\n","Iteration 686, loss = 0.31832031\n","Iteration 687, loss = 0.31749566\n","Iteration 688, loss = 0.31651867\n","Iteration 689, loss = 0.31571609\n","Iteration 690, loss = 0.31483244\n","Iteration 691, loss = 0.31390089\n","Iteration 692, loss = 0.31308591\n","Iteration 693, loss = 0.31218938\n","Iteration 694, loss = 0.31126019\n","Iteration 695, loss = 0.31038875\n","Iteration 696, loss = 0.30952744\n","Iteration 697, loss = 0.30866748\n","Iteration 698, loss = 0.30782596\n","Iteration 699, loss = 0.30696859\n","Iteration 700, loss = 0.30606181\n","Iteration 701, loss = 0.30533609\n","Iteration 702, loss = 0.30440531\n","Iteration 703, loss = 0.30355351\n","Iteration 704, loss = 0.30261506\n","Iteration 705, loss = 0.30182015\n","Iteration 706, loss = 0.30102462\n","Iteration 707, loss = 0.30031395\n","Iteration 708, loss = 0.29936983\n","Iteration 709, loss = 0.29845206\n","Iteration 710, loss = 0.29773987\n","Iteration 711, loss = 0.29688955\n","Iteration 712, loss = 0.29607740\n","Iteration 713, loss = 0.29533445\n","Iteration 714, loss = 0.29451224\n","Iteration 715, loss = 0.29355438\n","Iteration 716, loss = 0.29269812\n","Iteration 717, loss = 0.29201689\n","Iteration 718, loss = 0.29108732\n","Iteration 719, loss = 0.29017682\n","Iteration 720, loss = 0.28946635\n","Iteration 721, loss = 0.28867188\n","Iteration 722, loss = 0.28776116\n","Iteration 723, loss = 0.28698065\n","Iteration 724, loss = 0.28640860\n","Iteration 725, loss = 0.28535909\n","Iteration 726, loss = 0.28453587\n","Iteration 727, loss = 0.28382093\n","Iteration 728, loss = 0.28298811\n","Iteration 729, loss = 0.28211873\n","Iteration 730, loss = 0.28137216\n","Iteration 731, loss = 0.28057382\n","Iteration 732, loss = 0.27983417\n","Iteration 733, loss = 0.27909631\n","Iteration 734, loss = 0.27824723\n","Iteration 735, loss = 0.27766114\n","Iteration 736, loss = 0.27667540\n","Iteration 737, loss = 0.27593048\n","Iteration 738, loss = 0.27516717\n","Iteration 739, loss = 0.27432637\n","Iteration 740, loss = 0.27383445\n","Iteration 741, loss = 0.27282446\n","Iteration 742, loss = 0.27200144\n","Iteration 743, loss = 0.27132565\n","Iteration 744, loss = 0.27053457\n","Iteration 745, loss = 0.26985718\n","Iteration 746, loss = 0.26898123\n","Iteration 747, loss = 0.26857527\n","Iteration 748, loss = 0.26756109\n","Iteration 749, loss = 0.26683417\n","Iteration 750, loss = 0.26603603\n","Iteration 751, loss = 0.26527998\n","Iteration 752, loss = 0.26450537\n","Iteration 753, loss = 0.26383771\n","Iteration 754, loss = 0.26313357\n","Iteration 755, loss = 0.26233974\n","Iteration 756, loss = 0.26161190\n","Iteration 757, loss = 0.26086153\n","Iteration 758, loss = 0.26044461\n","Iteration 759, loss = 0.25945002\n","Iteration 760, loss = 0.25881822\n","Iteration 761, loss = 0.25809155\n","Iteration 762, loss = 0.25722859\n","Iteration 763, loss = 0.25656134\n","Iteration 764, loss = 0.25586814\n","Iteration 765, loss = 0.25532919\n","Iteration 766, loss = 0.25449840\n","Iteration 767, loss = 0.25369196\n","Iteration 768, loss = 0.25298682\n","Iteration 769, loss = 0.25264294\n","Iteration 770, loss = 0.25149520\n","Iteration 771, loss = 0.25082251\n","Iteration 772, loss = 0.25025883\n","Iteration 773, loss = 0.24959488\n","Iteration 774, loss = 0.24878991\n","Iteration 775, loss = 0.24818762\n","Iteration 776, loss = 0.24752301\n","Iteration 777, loss = 0.24659756\n","Iteration 778, loss = 0.24590045\n","Iteration 779, loss = 0.24527351\n","Iteration 780, loss = 0.24479356\n","Iteration 781, loss = 0.24403840\n","Iteration 782, loss = 0.24328944\n","Iteration 783, loss = 0.24263031\n","Iteration 784, loss = 0.24192129\n","Iteration 785, loss = 0.24129736\n","Iteration 786, loss = 0.24061658\n","Iteration 787, loss = 0.23996880\n","Iteration 788, loss = 0.23938610\n","Iteration 789, loss = 0.23875659\n","Iteration 790, loss = 0.23795716\n","Iteration 791, loss = 0.23737263\n","Iteration 792, loss = 0.23689534\n","Iteration 793, loss = 0.23606362\n","Iteration 794, loss = 0.23531907\n","Iteration 795, loss = 0.23467830\n","Iteration 796, loss = 0.23402086\n","Iteration 797, loss = 0.23347680\n","Iteration 798, loss = 0.23285191\n","Iteration 799, loss = 0.23215344\n","Iteration 800, loss = 0.23148058\n","Iteration 801, loss = 0.23111045\n","Iteration 802, loss = 0.23048844\n","Iteration 803, loss = 0.22958942\n","Iteration 804, loss = 0.22897428\n","Iteration 805, loss = 0.22852156\n","Iteration 806, loss = 0.22782626\n","Iteration 807, loss = 0.22707507\n","Iteration 808, loss = 0.22647452\n","Iteration 809, loss = 0.22583003\n","Iteration 810, loss = 0.22524921\n","Iteration 811, loss = 0.22459416\n","Iteration 812, loss = 0.22420131\n","Iteration 813, loss = 0.22373302\n","Iteration 814, loss = 0.22283731\n","Iteration 815, loss = 0.22223217\n","Iteration 816, loss = 0.22171383\n","Iteration 817, loss = 0.22096012\n","Iteration 818, loss = 0.22041408\n","Iteration 819, loss = 0.21985790\n","Iteration 820, loss = 0.21940152\n","Iteration 821, loss = 0.21867641\n","Iteration 822, loss = 0.21804917\n","Iteration 823, loss = 0.21750493\n","Iteration 824, loss = 0.21692924\n","Iteration 825, loss = 0.21646743\n","Iteration 826, loss = 0.21575707\n","Iteration 827, loss = 0.21513688\n","Iteration 828, loss = 0.21470501\n","Iteration 829, loss = 0.21404469\n","Iteration 830, loss = 0.21348554\n","Iteration 831, loss = 0.21308530\n","Iteration 832, loss = 0.21243358\n","Iteration 833, loss = 0.21214393\n","Iteration 834, loss = 0.21134065\n","Iteration 835, loss = 0.21113092\n","Iteration 836, loss = 0.21012164\n","Iteration 837, loss = 0.20953344\n","Iteration 838, loss = 0.20910086\n","Iteration 839, loss = 0.20849494\n","Iteration 840, loss = 0.20793046\n","Iteration 841, loss = 0.20741186\n","Iteration 842, loss = 0.20697088\n","Iteration 843, loss = 0.20623211\n","Iteration 844, loss = 0.20578496\n","Iteration 845, loss = 0.20527053\n","Iteration 846, loss = 0.20470197\n","Iteration 847, loss = 0.20415220\n","Iteration 848, loss = 0.20357221\n","Iteration 849, loss = 0.20334360\n","Iteration 850, loss = 0.20263933\n","Iteration 851, loss = 0.20206633\n","Iteration 852, loss = 0.20152310\n","Iteration 853, loss = 0.20106767\n","Iteration 854, loss = 0.20048455\n","Iteration 855, loss = 0.20039958\n","Iteration 856, loss = 0.19951908\n","Iteration 857, loss = 0.19898362\n","Iteration 858, loss = 0.19840198\n","Iteration 859, loss = 0.19818747\n","Iteration 860, loss = 0.19748516\n","Iteration 861, loss = 0.19694392\n","Iteration 862, loss = 0.19641704\n","Iteration 863, loss = 0.19602281\n","Iteration 864, loss = 0.19538116\n","Iteration 865, loss = 0.19495942\n","Iteration 866, loss = 0.19451252\n","Iteration 867, loss = 0.19408827\n","Iteration 868, loss = 0.19353368\n","Iteration 869, loss = 0.19300198\n","Iteration 870, loss = 0.19250615\n","Iteration 871, loss = 0.19200259\n","Iteration 872, loss = 0.19151062\n","Iteration 873, loss = 0.19112485\n","Iteration 874, loss = 0.19059969\n","Iteration 875, loss = 0.19045962\n","Iteration 876, loss = 0.18976610\n","Iteration 877, loss = 0.18920929\n","Iteration 878, loss = 0.18935047\n","Iteration 879, loss = 0.18828832\n","Iteration 880, loss = 0.18772852\n","Iteration 881, loss = 0.18752446\n","Iteration 882, loss = 0.18686575\n","Iteration 883, loss = 0.18656004\n","Iteration 884, loss = 0.18597536\n","Iteration 885, loss = 0.18544410\n","Iteration 886, loss = 0.18505483\n","Iteration 887, loss = 0.18461778\n","Iteration 888, loss = 0.18419369\n","Iteration 889, loss = 0.18376411\n","Iteration 890, loss = 0.18336684\n","Iteration 891, loss = 0.18285711\n","Iteration 892, loss = 0.18234748\n","Iteration 893, loss = 0.18187576\n","Iteration 894, loss = 0.18151724\n","Iteration 895, loss = 0.18109296\n","Iteration 896, loss = 0.18061517\n","Iteration 897, loss = 0.18038345\n","Iteration 898, loss = 0.17997878\n","Iteration 899, loss = 0.17946743\n","Iteration 900, loss = 0.17900208\n","Iteration 901, loss = 0.17871520\n","Iteration 902, loss = 0.17811551\n","Iteration 903, loss = 0.17769370\n","Iteration 904, loss = 0.17721740\n","Iteration 905, loss = 0.17686930\n","Iteration 906, loss = 0.17650505\n","Iteration 907, loss = 0.17610189\n","Iteration 908, loss = 0.17565245\n","Iteration 909, loss = 0.17519158\n","Iteration 910, loss = 0.17493451\n","Iteration 911, loss = 0.17446397\n","Iteration 912, loss = 0.17411177\n","Iteration 913, loss = 0.17359543\n","Iteration 914, loss = 0.17314659\n","Iteration 915, loss = 0.17272970\n","Iteration 916, loss = 0.17239136\n","Iteration 917, loss = 0.17198987\n","Iteration 918, loss = 0.17154626\n","Iteration 919, loss = 0.17127044\n","Iteration 920, loss = 0.17088644\n","Iteration 921, loss = 0.17053605\n","Iteration 922, loss = 0.17006035\n","Iteration 923, loss = 0.16962855\n","Iteration 924, loss = 0.16939200\n","Iteration 925, loss = 0.16883429\n","Iteration 926, loss = 0.16853136\n","Iteration 927, loss = 0.16819570\n","Iteration 928, loss = 0.16802136\n","Iteration 929, loss = 0.16745474\n","Iteration 930, loss = 0.16695770\n","Iteration 931, loss = 0.16664991\n","Iteration 932, loss = 0.16634700\n","Iteration 933, loss = 0.16593562\n","Iteration 934, loss = 0.16555856\n","Iteration 935, loss = 0.16514653\n","Iteration 936, loss = 0.16487604\n","Iteration 937, loss = 0.16441646\n","Iteration 938, loss = 0.16418826\n","Iteration 939, loss = 0.16386788\n","Iteration 940, loss = 0.16332185\n","Iteration 941, loss = 0.16297673\n","Iteration 942, loss = 0.16260303\n","Iteration 943, loss = 0.16239024\n","Iteration 944, loss = 0.16204316\n","Iteration 945, loss = 0.16180257\n","Iteration 946, loss = 0.16132644\n","Iteration 947, loss = 0.16098342\n","Iteration 948, loss = 0.16053744\n","Iteration 949, loss = 0.16033939\n","Iteration 950, loss = 0.15987376\n","Iteration 951, loss = 0.15967506\n","Iteration 952, loss = 0.15932826\n","Iteration 953, loss = 0.15892942\n","Iteration 954, loss = 0.15909501\n","Iteration 955, loss = 0.15852977\n","Iteration 956, loss = 0.15792922\n","Iteration 957, loss = 0.15750348\n","Iteration 958, loss = 0.15717595\n","Iteration 959, loss = 0.15689575\n","Iteration 960, loss = 0.15654151\n","Iteration 961, loss = 0.15636508\n","Iteration 962, loss = 0.15615400\n","Iteration 963, loss = 0.15553201\n","Iteration 964, loss = 0.15527842\n","Iteration 965, loss = 0.15488714\n","Iteration 966, loss = 0.15539895\n","Iteration 967, loss = 0.15443004\n","Iteration 968, loss = 0.15410288\n","Iteration 969, loss = 0.15363319\n","Iteration 970, loss = 0.15328337\n","Iteration 971, loss = 0.15345543\n","Iteration 972, loss = 0.15334505\n","Iteration 973, loss = 0.15284781\n","Iteration 974, loss = 0.15225937\n","Iteration 975, loss = 0.15211742\n","Iteration 976, loss = 0.15162359\n","Iteration 977, loss = 0.15131415\n","Iteration 978, loss = 0.15089741\n","Iteration 979, loss = 0.15066755\n","Iteration 980, loss = 0.15043327\n","Iteration 981, loss = 0.15011504\n","Iteration 982, loss = 0.14974381\n","Iteration 983, loss = 0.14950532\n","Iteration 984, loss = 0.14943159\n","Iteration 985, loss = 0.14910051\n","Iteration 986, loss = 0.14860152\n","Iteration 987, loss = 0.14831977\n","Iteration 988, loss = 0.14862738\n","Iteration 989, loss = 0.14821384\n","Iteration 990, loss = 0.14746510\n","Iteration 991, loss = 0.14714759\n","Iteration 992, loss = 0.14690207\n","Iteration 993, loss = 0.14699694\n","Iteration 994, loss = 0.14634477\n","Iteration 995, loss = 0.14596766\n","Iteration 996, loss = 0.14582820\n","Iteration 997, loss = 0.14543880\n","Iteration 998, loss = 0.14523620\n","Iteration 999, loss = 0.14488493\n","Iteration 1000, loss = 0.14485679\n","Iteration 1001, loss = 0.14452844\n","Iteration 1002, loss = 0.14430929\n","Iteration 1003, loss = 0.14384619\n","Iteration 1004, loss = 0.14361849\n","Iteration 1005, loss = 0.14334997\n","Iteration 1006, loss = 0.14324151\n","Iteration 1007, loss = 0.14284586\n","Iteration 1008, loss = 0.14253541\n","Iteration 1009, loss = 0.14236077\n","Iteration 1010, loss = 0.14232017\n","Iteration 1011, loss = 0.14170686\n","Iteration 1012, loss = 0.14161579\n","Iteration 1013, loss = 0.14130047\n","Iteration 1014, loss = 0.14127383\n","Iteration 1015, loss = 0.14073832\n","Iteration 1016, loss = 0.14044428\n","Iteration 1017, loss = 0.14028118\n","Iteration 1018, loss = 0.14008959\n","Iteration 1019, loss = 0.13965630\n","Iteration 1020, loss = 0.13949151\n","Iteration 1021, loss = 0.13918501\n","Iteration 1022, loss = 0.13892052\n","Iteration 1023, loss = 0.13869176\n","Iteration 1024, loss = 0.13848310\n","Iteration 1025, loss = 0.13835515\n","Iteration 1026, loss = 0.13803094\n","Iteration 1027, loss = 0.13773073\n","Iteration 1028, loss = 0.13749549\n","Iteration 1029, loss = 0.13728126\n","Iteration 1030, loss = 0.13694195\n","Iteration 1031, loss = 0.13675071\n","Iteration 1032, loss = 0.13686509\n","Iteration 1033, loss = 0.13636935\n","Iteration 1034, loss = 0.13610666\n","Iteration 1035, loss = 0.13599510\n","Iteration 1036, loss = 0.13556715\n","Iteration 1037, loss = 0.13545328\n","Iteration 1038, loss = 0.13507763\n","Iteration 1039, loss = 0.13509134\n","Iteration 1040, loss = 0.13461227\n","Iteration 1041, loss = 0.13438838\n","Iteration 1042, loss = 0.13410097\n","Iteration 1043, loss = 0.13396173\n","Iteration 1044, loss = 0.13366087\n","Iteration 1045, loss = 0.13363153\n","Iteration 1046, loss = 0.13341054\n","Iteration 1047, loss = 0.13317212\n","Iteration 1048, loss = 0.13282586\n","Iteration 1049, loss = 0.13300554\n","Iteration 1050, loss = 0.13249295\n","Iteration 1051, loss = 0.13223607\n","Iteration 1052, loss = 0.13191759\n","Iteration 1053, loss = 0.13188801\n","Iteration 1054, loss = 0.13147950\n","Iteration 1055, loss = 0.13125611\n","Iteration 1056, loss = 0.13097254\n","Iteration 1057, loss = 0.13087685\n","Iteration 1058, loss = 0.13056412\n","Iteration 1059, loss = 0.13042436\n","Iteration 1060, loss = 0.13016216\n","Iteration 1061, loss = 0.13013951\n","Iteration 1062, loss = 0.12971591\n","Iteration 1063, loss = 0.12983102\n","Iteration 1064, loss = 0.12935135\n","Iteration 1065, loss = 0.12940810\n","Iteration 1066, loss = 0.12891063\n","Iteration 1067, loss = 0.12878921\n","Iteration 1068, loss = 0.12887052\n","Iteration 1069, loss = 0.12830460\n","Iteration 1070, loss = 0.12798724\n","Iteration 1071, loss = 0.12779803\n","Iteration 1072, loss = 0.12757619\n","Iteration 1073, loss = 0.12745630\n","Iteration 1074, loss = 0.12745526\n","Iteration 1075, loss = 0.12710583\n","Iteration 1076, loss = 0.12717639\n","Iteration 1077, loss = 0.12676476\n","Iteration 1078, loss = 0.12664194\n","Iteration 1079, loss = 0.12643641\n","Iteration 1080, loss = 0.12650201\n","Iteration 1081, loss = 0.12616852\n","Iteration 1082, loss = 0.12561190\n","Iteration 1083, loss = 0.12544750\n","Iteration 1084, loss = 0.12529163\n","Iteration 1085, loss = 0.12511808\n","Iteration 1086, loss = 0.12498110\n","Iteration 1087, loss = 0.12474367\n","Iteration 1088, loss = 0.12452542\n","Iteration 1089, loss = 0.12439763\n","Iteration 1090, loss = 0.12413314\n","Iteration 1091, loss = 0.12397711\n","Iteration 1092, loss = 0.12390835\n","Iteration 1093, loss = 0.12353974\n","Iteration 1094, loss = 0.12352017\n","Iteration 1095, loss = 0.12319400\n","Iteration 1096, loss = 0.12299382\n","Iteration 1097, loss = 0.12288829\n","Iteration 1098, loss = 0.12263225\n","Iteration 1099, loss = 0.12239763\n","Iteration 1100, loss = 0.12223234\n","Iteration 1101, loss = 0.12226143\n","Iteration 1102, loss = 0.12215359\n","Iteration 1103, loss = 0.12179174\n","Iteration 1104, loss = 0.12175808\n","Iteration 1105, loss = 0.12169085\n","Iteration 1106, loss = 0.12116448\n","Iteration 1107, loss = 0.12116603\n","Iteration 1108, loss = 0.12081474\n","Iteration 1109, loss = 0.12098019\n","Iteration 1110, loss = 0.12059837\n","Iteration 1111, loss = 0.12032368\n","Iteration 1112, loss = 0.12021807\n","Iteration 1113, loss = 0.11997285\n","Iteration 1114, loss = 0.11978319\n","Iteration 1115, loss = 0.12038548\n","Iteration 1116, loss = 0.11947877\n","Iteration 1117, loss = 0.11921526\n","Iteration 1118, loss = 0.11921649\n","Iteration 1119, loss = 0.11915502\n","Iteration 1120, loss = 0.11884813\n","Iteration 1121, loss = 0.11861464\n","Iteration 1122, loss = 0.11857902\n","Iteration 1123, loss = 0.11857130\n","Iteration 1124, loss = 0.11805532\n","Iteration 1125, loss = 0.11803618\n","Iteration 1126, loss = 0.11806334\n","Iteration 1127, loss = 0.11759314\n","Iteration 1128, loss = 0.11759358\n","Iteration 1129, loss = 0.11739567\n","Iteration 1130, loss = 0.11705827\n","Iteration 1131, loss = 0.11693358\n","Iteration 1132, loss = 0.11678137\n","Iteration 1133, loss = 0.11665812\n","Iteration 1134, loss = 0.11653452\n","Iteration 1135, loss = 0.11645391\n","Iteration 1136, loss = 0.11621383\n","Iteration 1137, loss = 0.11632327\n","Iteration 1138, loss = 0.11593357\n","Iteration 1139, loss = 0.11588092\n","Iteration 1140, loss = 0.11563664\n","Iteration 1141, loss = 0.11540424\n","Iteration 1142, loss = 0.11535330\n","Iteration 1143, loss = 0.11508848\n","Iteration 1144, loss = 0.11491221\n","Iteration 1145, loss = 0.11478290\n","Iteration 1146, loss = 0.11461843\n","Iteration 1147, loss = 0.11447679\n","Iteration 1148, loss = 0.11439162\n","Iteration 1149, loss = 0.11413061\n","Iteration 1150, loss = 0.11396401\n","Iteration 1151, loss = 0.11384243\n","Iteration 1152, loss = 0.11380192\n","Iteration 1153, loss = 0.11357700\n","Iteration 1154, loss = 0.11351083\n","Iteration 1155, loss = 0.11389810\n","Iteration 1156, loss = 0.11316912\n","Iteration 1157, loss = 0.11309407\n","Iteration 1158, loss = 0.11276070\n","Iteration 1159, loss = 0.11262156\n","Iteration 1160, loss = 0.11246583\n","Iteration 1161, loss = 0.11241084\n","Iteration 1162, loss = 0.11231066\n","Iteration 1163, loss = 0.11201444\n","Iteration 1164, loss = 0.11195741\n","Iteration 1165, loss = 0.11211631\n","Iteration 1166, loss = 0.11163066\n","Iteration 1167, loss = 0.11154559\n","Iteration 1168, loss = 0.11148586\n","Iteration 1169, loss = 0.11117102\n","Iteration 1170, loss = 0.11113139\n","Iteration 1171, loss = 0.11088539\n","Iteration 1172, loss = 0.11094040\n","Iteration 1173, loss = 0.11067867\n","Iteration 1174, loss = 0.11085986\n","Iteration 1175, loss = 0.11035990\n","Iteration 1176, loss = 0.11017933\n","Iteration 1177, loss = 0.11045917\n","Iteration 1178, loss = 0.10997194\n","Iteration 1179, loss = 0.10975305\n","Iteration 1180, loss = 0.10966069\n","Iteration 1181, loss = 0.10949178\n","Iteration 1182, loss = 0.10939635\n","Iteration 1183, loss = 0.10929442\n","Iteration 1184, loss = 0.10923441\n","Iteration 1185, loss = 0.10906471\n","Iteration 1186, loss = 0.10880806\n","Iteration 1187, loss = 0.10910653\n","Iteration 1188, loss = 0.10860084\n","Iteration 1189, loss = 0.10837822\n","Iteration 1190, loss = 0.10829441\n","Iteration 1191, loss = 0.10844778\n","Iteration 1192, loss = 0.10814605\n","Iteration 1193, loss = 0.10785841\n","Iteration 1194, loss = 0.10780465\n","Iteration 1195, loss = 0.10762918\n","Iteration 1196, loss = 0.10772163\n","Iteration 1197, loss = 0.10762040\n","Iteration 1198, loss = 0.10723029\n","Iteration 1199, loss = 0.10710014\n","Iteration 1200, loss = 0.10702186\n","Iteration 1201, loss = 0.10708125\n","Iteration 1202, loss = 0.10673436\n","Iteration 1203, loss = 0.10665285\n","Iteration 1204, loss = 0.10664644\n","Iteration 1205, loss = 0.10641004\n","Iteration 1206, loss = 0.10628278\n","Iteration 1207, loss = 0.10618061\n","Iteration 1208, loss = 0.10595861\n","Iteration 1209, loss = 0.10637658\n","Iteration 1210, loss = 0.10578007\n","Iteration 1211, loss = 0.10567420\n","Iteration 1212, loss = 0.10553089\n","Iteration 1213, loss = 0.10547721\n","Iteration 1214, loss = 0.10522017\n","Iteration 1215, loss = 0.10520002\n","Iteration 1216, loss = 0.10493683\n","Iteration 1217, loss = 0.10496395\n","Iteration 1218, loss = 0.10486535\n","Iteration 1219, loss = 0.10470491\n","Iteration 1220, loss = 0.10452753\n","Iteration 1221, loss = 0.10442005\n","Iteration 1222, loss = 0.10433867\n","Iteration 1223, loss = 0.10418483\n","Iteration 1224, loss = 0.10411102\n","Iteration 1225, loss = 0.10392774\n","Iteration 1226, loss = 0.10383305\n","Iteration 1227, loss = 0.10393921\n","Iteration 1228, loss = 0.10367073\n","Iteration 1229, loss = 0.10371484\n","Iteration 1230, loss = 0.10360877\n","Iteration 1231, loss = 0.10393140\n","Iteration 1232, loss = 0.10312767\n","Iteration 1233, loss = 0.10309393\n","Iteration 1234, loss = 0.10290133\n","Iteration 1235, loss = 0.10296793\n","Iteration 1236, loss = 0.10261764\n","Iteration 1237, loss = 0.10247203\n","Iteration 1238, loss = 0.10258926\n","Iteration 1239, loss = 0.10243092\n","Iteration 1240, loss = 0.10228544\n","Iteration 1241, loss = 0.10214081\n","Iteration 1242, loss = 0.10220185\n","Iteration 1243, loss = 0.10192654\n","Iteration 1244, loss = 0.10174819\n","Iteration 1245, loss = 0.10168236\n","Iteration 1246, loss = 0.10153630\n","Iteration 1247, loss = 0.10137999\n","Iteration 1248, loss = 0.10136432\n","Iteration 1249, loss = 0.10158558\n","Iteration 1250, loss = 0.10113909\n","Iteration 1251, loss = 0.10112874\n","Iteration 1252, loss = 0.10104773\n","Iteration 1253, loss = 0.10075576\n","Iteration 1254, loss = 0.10063181\n","Iteration 1255, loss = 0.10052809\n","Iteration 1256, loss = 0.10047073\n","Iteration 1257, loss = 0.10030097\n","Iteration 1258, loss = 0.10027383\n","Iteration 1259, loss = 0.10007460\n","Iteration 1260, loss = 0.10006427\n","Iteration 1261, loss = 0.10001889\n","Iteration 1262, loss = 0.09980826\n","Iteration 1263, loss = 0.10000824\n","Iteration 1264, loss = 0.09954538\n","Iteration 1265, loss = 0.09943857\n","Iteration 1266, loss = 0.09981571\n","Iteration 1267, loss = 0.09943248\n","Iteration 1268, loss = 0.09919955\n","Iteration 1269, loss = 0.09905411\n","Iteration 1270, loss = 0.09925038\n","Iteration 1271, loss = 0.09893471\n","Iteration 1272, loss = 0.09879566\n","Iteration 1273, loss = 0.09874135\n","Iteration 1274, loss = 0.09849521\n","Iteration 1275, loss = 0.09890953\n","Iteration 1276, loss = 0.09849510\n","Iteration 1277, loss = 0.09821653\n","Iteration 1278, loss = 0.09822509\n","Iteration 1279, loss = 0.09811922\n","Iteration 1280, loss = 0.09804996\n","Iteration 1281, loss = 0.09790881\n","Iteration 1282, loss = 0.09809916\n","Iteration 1283, loss = 0.09772012\n","Iteration 1284, loss = 0.09777472\n","Iteration 1285, loss = 0.09756413\n","Iteration 1286, loss = 0.09750972\n","Iteration 1287, loss = 0.09737065\n","Iteration 1288, loss = 0.09726984\n","Iteration 1289, loss = 0.09736197\n","Iteration 1290, loss = 0.09708196\n","Iteration 1291, loss = 0.09691867\n","Iteration 1292, loss = 0.09683078\n","Iteration 1293, loss = 0.09679933\n","Iteration 1294, loss = 0.09659918\n","Iteration 1295, loss = 0.09650083\n","Iteration 1296, loss = 0.09634145\n","Iteration 1297, loss = 0.09625243\n","Iteration 1298, loss = 0.09626597\n","Iteration 1299, loss = 0.09621320\n","Iteration 1300, loss = 0.09608166\n","Iteration 1301, loss = 0.09595812\n","Iteration 1302, loss = 0.09607452\n","Iteration 1303, loss = 0.09568660\n","Iteration 1304, loss = 0.09599696\n","Iteration 1305, loss = 0.09544545\n","Iteration 1306, loss = 0.09565837\n","Iteration 1307, loss = 0.09560196\n","Iteration 1308, loss = 0.09554036\n","Iteration 1309, loss = 0.09527584\n","Iteration 1310, loss = 0.09516224\n","Iteration 1311, loss = 0.09495986\n","Iteration 1312, loss = 0.09528086\n","Iteration 1313, loss = 0.09495536\n","Iteration 1314, loss = 0.09483168\n","Iteration 1315, loss = 0.09467615\n","Iteration 1316, loss = 0.09451061\n","Iteration 1317, loss = 0.09446353\n","Iteration 1318, loss = 0.09440536\n","Iteration 1319, loss = 0.09426714\n","Iteration 1320, loss = 0.09422939\n","Iteration 1321, loss = 0.09449064\n","Iteration 1322, loss = 0.09396933\n","Iteration 1323, loss = 0.09391478\n","Iteration 1324, loss = 0.09397721\n","Iteration 1325, loss = 0.09464511\n","Iteration 1326, loss = 0.09395530\n","Iteration 1327, loss = 0.09371273\n","Iteration 1328, loss = 0.09355689\n","Iteration 1329, loss = 0.09368010\n","Iteration 1330, loss = 0.09349951\n","Iteration 1331, loss = 0.09327999\n","Iteration 1332, loss = 0.09340535\n","Iteration 1333, loss = 0.09311452\n","Iteration 1334, loss = 0.09302010\n","Iteration 1335, loss = 0.09294443\n","Iteration 1336, loss = 0.09301306\n","Iteration 1337, loss = 0.09283477\n","Iteration 1338, loss = 0.09289812\n","Iteration 1339, loss = 0.09248757\n","Iteration 1340, loss = 0.09241466\n","Iteration 1341, loss = 0.09241442\n","Iteration 1342, loss = 0.09239419\n","Iteration 1343, loss = 0.09237763\n","Iteration 1344, loss = 0.09220580\n","Iteration 1345, loss = 0.09219755\n","Iteration 1346, loss = 0.09206130\n","Iteration 1347, loss = 0.09190723\n","Iteration 1348, loss = 0.09209225\n","Iteration 1349, loss = 0.09173582\n","Iteration 1350, loss = 0.09183441\n","Iteration 1351, loss = 0.09159664\n","Iteration 1352, loss = 0.09147272\n","Iteration 1353, loss = 0.09153580\n","Iteration 1354, loss = 0.09130692\n","Iteration 1355, loss = 0.09143320\n","Iteration 1356, loss = 0.09129417\n","Iteration 1357, loss = 0.09118702\n","Iteration 1358, loss = 0.09125606\n","Iteration 1359, loss = 0.09097409\n","Iteration 1360, loss = 0.09102923\n","Iteration 1361, loss = 0.09077504\n","Iteration 1362, loss = 0.09094030\n","Iteration 1363, loss = 0.09059972\n","Iteration 1364, loss = 0.09066368\n","Iteration 1365, loss = 0.09044712\n","Iteration 1366, loss = 0.09041279\n","Iteration 1367, loss = 0.09034972\n","Iteration 1368, loss = 0.09023804\n","Iteration 1369, loss = 0.09033334\n","Iteration 1370, loss = 0.09021449\n","Iteration 1371, loss = 0.09017224\n","Iteration 1372, loss = 0.09022831\n","Iteration 1373, loss = 0.09009645\n","Iteration 1374, loss = 0.08981738\n","Iteration 1375, loss = 0.09028201\n","Iteration 1376, loss = 0.08979594\n","Iteration 1377, loss = 0.08962253\n","Iteration 1378, loss = 0.08949051\n","Iteration 1379, loss = 0.08945985\n","Iteration 1380, loss = 0.08931044\n","Iteration 1381, loss = 0.08922355\n","Iteration 1382, loss = 0.08931189\n","Iteration 1383, loss = 0.08911230\n","Iteration 1384, loss = 0.08913951\n","Iteration 1385, loss = 0.08899095\n","Iteration 1386, loss = 0.08893555\n","Iteration 1387, loss = 0.08887490\n","Iteration 1388, loss = 0.08873340\n","Iteration 1389, loss = 0.08887746\n","Iteration 1390, loss = 0.08860506\n","Iteration 1391, loss = 0.08867429\n","Iteration 1392, loss = 0.08888763\n","Iteration 1393, loss = 0.08865025\n","Iteration 1394, loss = 0.08831357\n","Iteration 1395, loss = 0.08873033\n","Iteration 1396, loss = 0.08826515\n","Iteration 1397, loss = 0.08821052\n","Iteration 1398, loss = 0.08815655\n","Iteration 1399, loss = 0.08795618\n","Iteration 1400, loss = 0.08785752\n","Iteration 1401, loss = 0.08777741\n","Iteration 1402, loss = 0.08773198\n","Iteration 1403, loss = 0.08761530\n","Iteration 1404, loss = 0.08766176\n","Iteration 1405, loss = 0.08749560\n","Iteration 1406, loss = 0.08753340\n","Iteration 1407, loss = 0.08735687\n","Iteration 1408, loss = 0.08747651\n","Iteration 1409, loss = 0.08723171\n","Iteration 1410, loss = 0.08738082\n","Iteration 1411, loss = 0.08717547\n","Iteration 1412, loss = 0.08700378\n","Iteration 1413, loss = 0.08694384\n","Iteration 1414, loss = 0.08699577\n","Iteration 1415, loss = 0.08687173\n","Iteration 1416, loss = 0.08704229\n","Iteration 1417, loss = 0.08668154\n","Iteration 1418, loss = 0.08660321\n","Iteration 1419, loss = 0.08654828\n","Iteration 1420, loss = 0.08663476\n","Iteration 1421, loss = 0.08662144\n","Iteration 1422, loss = 0.08643444\n","Iteration 1423, loss = 0.08626319\n","Iteration 1424, loss = 0.08619648\n","Iteration 1425, loss = 0.08621072\n","Iteration 1426, loss = 0.08620886\n","Iteration 1427, loss = 0.08606603\n","Iteration 1428, loss = 0.08601500\n","Iteration 1429, loss = 0.08590822\n","Iteration 1430, loss = 0.08586275\n","Iteration 1431, loss = 0.08582397\n","Iteration 1432, loss = 0.08591478\n","Iteration 1433, loss = 0.08566427\n","Iteration 1434, loss = 0.08564873\n","Iteration 1435, loss = 0.08561119\n","Iteration 1436, loss = 0.08538130\n","Iteration 1437, loss = 0.08562665\n","Iteration 1438, loss = 0.08529596\n","Iteration 1439, loss = 0.08553943\n","Iteration 1440, loss = 0.08539030\n","Iteration 1441, loss = 0.08518398\n","Iteration 1442, loss = 0.08507884\n","Iteration 1443, loss = 0.08502845\n","Iteration 1444, loss = 0.08495166\n","Iteration 1445, loss = 0.08489869\n","Iteration 1446, loss = 0.08481328\n","Iteration 1447, loss = 0.08485433\n","Iteration 1448, loss = 0.08482797\n","Iteration 1449, loss = 0.08456596\n","Iteration 1450, loss = 0.08476215\n","Iteration 1451, loss = 0.08463925\n","Iteration 1452, loss = 0.08492353\n","Iteration 1453, loss = 0.08432891\n","Iteration 1454, loss = 0.08458439\n","Iteration 1455, loss = 0.08441970\n","Iteration 1456, loss = 0.08417298\n","Iteration 1457, loss = 0.08417797\n","Iteration 1458, loss = 0.08421468\n","Iteration 1459, loss = 0.08403895\n","Iteration 1460, loss = 0.08388983\n","Iteration 1461, loss = 0.08386021\n","Iteration 1462, loss = 0.08409736\n","Iteration 1463, loss = 0.08382666\n","Iteration 1464, loss = 0.08374965\n","Iteration 1465, loss = 0.08358637\n","Iteration 1466, loss = 0.08403457\n","Iteration 1467, loss = 0.08374263\n","Iteration 1468, loss = 0.08352675\n","Iteration 1469, loss = 0.08336575\n","Iteration 1470, loss = 0.08350303\n","Iteration 1471, loss = 0.08328489\n","Iteration 1472, loss = 0.08333110\n","Iteration 1473, loss = 0.08332341\n","Iteration 1474, loss = 0.08319494\n","Iteration 1475, loss = 0.08323734\n","Iteration 1476, loss = 0.08357586\n","Iteration 1477, loss = 0.08285053\n","Iteration 1478, loss = 0.08339887\n","Iteration 1479, loss = 0.08290066\n","Iteration 1480, loss = 0.08295984\n","Iteration 1481, loss = 0.08266448\n","Iteration 1482, loss = 0.08277755\n","Iteration 1483, loss = 0.08271961\n","Iteration 1484, loss = 0.08254835\n","Iteration 1485, loss = 0.08264239\n","Iteration 1486, loss = 0.08263971\n","Iteration 1487, loss = 0.08241343\n","Iteration 1488, loss = 0.08232319\n","Iteration 1489, loss = 0.08230924\n","Iteration 1490, loss = 0.08235564\n","Iteration 1491, loss = 0.08215572\n","Iteration 1492, loss = 0.08221005\n","Iteration 1493, loss = 0.08204748\n","Iteration 1494, loss = 0.08211447\n","Iteration 1495, loss = 0.08190666\n","Iteration 1496, loss = 0.08192192\n","Iteration 1497, loss = 0.08192004\n","Iteration 1498, loss = 0.08171986\n","Iteration 1499, loss = 0.08168402\n","Iteration 1500, loss = 0.08171142\n","Iteration 1501, loss = 0.08159588\n","Iteration 1502, loss = 0.08175387\n","Iteration 1503, loss = 0.08151877\n","Iteration 1504, loss = 0.08141991\n","Iteration 1505, loss = 0.08132366\n","Iteration 1506, loss = 0.08126703\n","Iteration 1507, loss = 0.08154437\n","Iteration 1508, loss = 0.08153746\n","Iteration 1509, loss = 0.08115777\n","Iteration 1510, loss = 0.08112835\n","Iteration 1511, loss = 0.08104465\n","Iteration 1512, loss = 0.08096577\n","Iteration 1513, loss = 0.08086159\n","Iteration 1514, loss = 0.08123752\n","Iteration 1515, loss = 0.08110718\n","Iteration 1516, loss = 0.08147967\n","Iteration 1517, loss = 0.08074308\n","Iteration 1518, loss = 0.08063572\n","Iteration 1519, loss = 0.08076761\n","Iteration 1520, loss = 0.08073731\n","Iteration 1521, loss = 0.08054823\n","Iteration 1522, loss = 0.08051909\n","Iteration 1523, loss = 0.08084075\n","Iteration 1524, loss = 0.08053321\n","Iteration 1525, loss = 0.08058055\n","Iteration 1526, loss = 0.08038198\n","Iteration 1527, loss = 0.08020655\n","Iteration 1528, loss = 0.08041754\n","Iteration 1529, loss = 0.08036712\n","Iteration 1530, loss = 0.08021243\n","Iteration 1531, loss = 0.08002031\n","Iteration 1532, loss = 0.07995363\n","Iteration 1533, loss = 0.08007550\n","Iteration 1534, loss = 0.07984148\n","Iteration 1535, loss = 0.08002473\n","Iteration 1536, loss = 0.08009820\n","Iteration 1537, loss = 0.08012423\n","Iteration 1538, loss = 0.07977522\n","Iteration 1539, loss = 0.07988881\n","Iteration 1540, loss = 0.07966381\n","Iteration 1541, loss = 0.07957115\n","Iteration 1542, loss = 0.07943224\n","Iteration 1543, loss = 0.07947717\n","Iteration 1544, loss = 0.07932866\n","Iteration 1545, loss = 0.07926924\n","Iteration 1546, loss = 0.07923339\n","Iteration 1547, loss = 0.07928219\n","Iteration 1548, loss = 0.07923356\n","Iteration 1549, loss = 0.07923098\n","Iteration 1550, loss = 0.07913558\n","Iteration 1551, loss = 0.07913100\n","Iteration 1552, loss = 0.07897198\n","Iteration 1553, loss = 0.07897049\n","Iteration 1554, loss = 0.07893077\n","Iteration 1555, loss = 0.07890126\n","Iteration 1556, loss = 0.07888769\n","Iteration 1557, loss = 0.07877955\n","Iteration 1558, loss = 0.07880798\n","Iteration 1559, loss = 0.07866409\n","Iteration 1560, loss = 0.07861366\n","Iteration 1561, loss = 0.07864271\n","Iteration 1562, loss = 0.07850012\n","Iteration 1563, loss = 0.07840727\n","Iteration 1564, loss = 0.07870018\n","Iteration 1565, loss = 0.07841653\n","Iteration 1566, loss = 0.07849141\n","Iteration 1567, loss = 0.07829196\n","Iteration 1568, loss = 0.07834770\n","Iteration 1569, loss = 0.07817166\n","Iteration 1570, loss = 0.07822068\n","Iteration 1571, loss = 0.07818609\n","Iteration 1572, loss = 0.07798121\n","Iteration 1573, loss = 0.07816421\n","Iteration 1574, loss = 0.07807237\n","Iteration 1575, loss = 0.07791677\n","Iteration 1576, loss = 0.07812044\n","Iteration 1577, loss = 0.07796139\n","Iteration 1578, loss = 0.07781295\n","Iteration 1579, loss = 0.07772329\n","Iteration 1580, loss = 0.07766987\n","Iteration 1581, loss = 0.07771072\n","Iteration 1582, loss = 0.07778909\n","Iteration 1583, loss = 0.07759706\n","Iteration 1584, loss = 0.07755096\n","Iteration 1585, loss = 0.07765855\n","Iteration 1586, loss = 0.07769297\n","Iteration 1587, loss = 0.07788126\n","Iteration 1588, loss = 0.07739667\n","Iteration 1589, loss = 0.07743540\n","Iteration 1590, loss = 0.07729870\n","Iteration 1591, loss = 0.07732196\n","Iteration 1592, loss = 0.07713623\n","Iteration 1593, loss = 0.07734334\n","Iteration 1594, loss = 0.07701461\n","Iteration 1595, loss = 0.07695382\n","Iteration 1596, loss = 0.07699385\n","Iteration 1597, loss = 0.07700046\n","Iteration 1598, loss = 0.07682284\n","Iteration 1599, loss = 0.07694101\n","Iteration 1600, loss = 0.07692389\n","Iteration 1601, loss = 0.07678580\n","Iteration 1602, loss = 0.07701436\n","Iteration 1603, loss = 0.07668135\n","Iteration 1604, loss = 0.07661044\n","Iteration 1605, loss = 0.07725214\n","Iteration 1606, loss = 0.07647655\n","Iteration 1607, loss = 0.07718120\n","Iteration 1608, loss = 0.07651105\n","Iteration 1609, loss = 0.07648516\n","Iteration 1610, loss = 0.07646458\n","Iteration 1611, loss = 0.07642685\n","Iteration 1612, loss = 0.07640230\n","Iteration 1613, loss = 0.07626814\n","Iteration 1614, loss = 0.07628588\n","Iteration 1615, loss = 0.07712397\n","Iteration 1616, loss = 0.07609881\n","Iteration 1617, loss = 0.07627634\n","Iteration 1618, loss = 0.07607467\n","Iteration 1619, loss = 0.07649683\n","Iteration 1620, loss = 0.07633543\n","Iteration 1621, loss = 0.07620223\n","Iteration 1622, loss = 0.07603074\n","Iteration 1623, loss = 0.07588281\n","Iteration 1624, loss = 0.07591194\n","Iteration 1625, loss = 0.07566902\n","Iteration 1626, loss = 0.07573741\n","Iteration 1627, loss = 0.07568928\n","Iteration 1628, loss = 0.07570526\n","Iteration 1629, loss = 0.07639321\n","Iteration 1630, loss = 0.07561396\n","Iteration 1631, loss = 0.07540907\n","Iteration 1632, loss = 0.07548872\n","Iteration 1633, loss = 0.07545633\n","Iteration 1634, loss = 0.07557548\n","Iteration 1635, loss = 0.07556579\n","Iteration 1636, loss = 0.07547264\n","Iteration 1637, loss = 0.07579971\n","Iteration 1638, loss = 0.07530728\n","Iteration 1639, loss = 0.07517815\n","Iteration 1640, loss = 0.07530129\n","Iteration 1641, loss = 0.07509816\n","Iteration 1642, loss = 0.07535789\n","Iteration 1643, loss = 0.07521378\n","Iteration 1644, loss = 0.07528463\n","Iteration 1645, loss = 0.07506311\n","Iteration 1646, loss = 0.07502539\n","Iteration 1647, loss = 0.07493802\n","Iteration 1648, loss = 0.07514579\n","Iteration 1649, loss = 0.07521573\n","Iteration 1650, loss = 0.07494764\n","Iteration 1651, loss = 0.07476999\n","Iteration 1652, loss = 0.07477805\n","Iteration 1653, loss = 0.07463990\n","Iteration 1654, loss = 0.07474758\n","Iteration 1655, loss = 0.07458826\n","Iteration 1656, loss = 0.07471299\n","Iteration 1657, loss = 0.07463835\n","Iteration 1658, loss = 0.07450383\n","Iteration 1659, loss = 0.07449341\n","Iteration 1660, loss = 0.07457030\n","Iteration 1661, loss = 0.07482364\n","Iteration 1662, loss = 0.07496719\n","Iteration 1663, loss = 0.07435638\n","Iteration 1664, loss = 0.07487879\n","Iteration 1665, loss = 0.07442764\n","Iteration 1666, loss = 0.07420350\n","Iteration 1667, loss = 0.07427277\n","Iteration 1668, loss = 0.07412696\n","Iteration 1669, loss = 0.07436578\n","Iteration 1670, loss = 0.07401590\n","Iteration 1671, loss = 0.07407967\n","Iteration 1672, loss = 0.07392254\n","Iteration 1673, loss = 0.07402722\n","Iteration 1674, loss = 0.07398186\n","Iteration 1675, loss = 0.07417683\n","Iteration 1676, loss = 0.07390854\n","Iteration 1677, loss = 0.07393704\n","Iteration 1678, loss = 0.07390875\n","Iteration 1679, loss = 0.07370624\n","Iteration 1680, loss = 0.07382073\n","Iteration 1681, loss = 0.07361435\n","Iteration 1682, loss = 0.07388696\n","Iteration 1683, loss = 0.07356062\n","Iteration 1684, loss = 0.07360044\n","Iteration 1685, loss = 0.07357008\n","Iteration 1686, loss = 0.07354167\n","Iteration 1687, loss = 0.07364922\n","Iteration 1688, loss = 0.07339141\n","Iteration 1689, loss = 0.07368991\n","Iteration 1690, loss = 0.07340690\n","Iteration 1691, loss = 0.07361488\n","Iteration 1692, loss = 0.07337369\n","Iteration 1693, loss = 0.07334327\n","Iteration 1694, loss = 0.07361757\n","Iteration 1695, loss = 0.07317487\n","Iteration 1696, loss = 0.07309336\n","Iteration 1697, loss = 0.07306263\n","Iteration 1698, loss = 0.07323480\n","Iteration 1699, loss = 0.07303333\n","Iteration 1700, loss = 0.07298107\n","Iteration 1701, loss = 0.07300851\n","Iteration 1702, loss = 0.07293391\n","Iteration 1703, loss = 0.07294005\n","Iteration 1704, loss = 0.07327595\n","Iteration 1705, loss = 0.07284948\n","Iteration 1706, loss = 0.07281852\n","Iteration 1707, loss = 0.07279572\n","Iteration 1708, loss = 0.07279392\n","Iteration 1709, loss = 0.07269079\n","Iteration 1710, loss = 0.07289097\n","Iteration 1711, loss = 0.07283209\n","Iteration 1712, loss = 0.07261817\n","Iteration 1713, loss = 0.07261003\n","Iteration 1714, loss = 0.07261865\n","Iteration 1715, loss = 0.07247658\n","Iteration 1716, loss = 0.07250078\n","Iteration 1717, loss = 0.07259970\n","Iteration 1718, loss = 0.07231021\n","Iteration 1719, loss = 0.07285381\n","Iteration 1720, loss = 0.07235247\n","Iteration 1721, loss = 0.07234418\n","Iteration 1722, loss = 0.07228700\n","Iteration 1723, loss = 0.07224228\n","Iteration 1724, loss = 0.07267185\n","Iteration 1725, loss = 0.07214638\n","Iteration 1726, loss = 0.07231306\n","Iteration 1727, loss = 0.07212689\n","Iteration 1728, loss = 0.07220838\n","Iteration 1729, loss = 0.07238634\n","Iteration 1730, loss = 0.07245179\n","Iteration 1731, loss = 0.07202280\n","Iteration 1732, loss = 0.07207719\n","Iteration 1733, loss = 0.07188088\n","Iteration 1734, loss = 0.07188146\n","Iteration 1735, loss = 0.07190582\n","Iteration 1736, loss = 0.07181874\n","Iteration 1737, loss = 0.07197384\n","Iteration 1738, loss = 0.07206339\n","Iteration 1739, loss = 0.07176749\n","Iteration 1740, loss = 0.07169238\n","Iteration 1741, loss = 0.07167068\n","Iteration 1742, loss = 0.07187287\n","Iteration 1743, loss = 0.07173719\n","Iteration 1744, loss = 0.07179266\n","Iteration 1745, loss = 0.07153748\n","Iteration 1746, loss = 0.07155098\n","Iteration 1747, loss = 0.07143215\n","Iteration 1748, loss = 0.07147684\n","Iteration 1749, loss = 0.07142235\n","Iteration 1750, loss = 0.07170791\n","Iteration 1751, loss = 0.07132752\n","Iteration 1752, loss = 0.07134916\n","Iteration 1753, loss = 0.07156758\n","Iteration 1754, loss = 0.07130457\n","Iteration 1755, loss = 0.07138493\n","Iteration 1756, loss = 0.07173638\n","Iteration 1757, loss = 0.07129658\n","Iteration 1758, loss = 0.07113215\n","Iteration 1759, loss = 0.07101605\n","Iteration 1760, loss = 0.07111494\n","Iteration 1761, loss = 0.07108521\n","Iteration 1762, loss = 0.07107988\n","Iteration 1763, loss = 0.07142959\n","Iteration 1764, loss = 0.07109553\n","Iteration 1765, loss = 0.07117118\n","Iteration 1766, loss = 0.07092406\n","Iteration 1767, loss = 0.07107368\n","Iteration 1768, loss = 0.07101493\n","Iteration 1769, loss = 0.07086398\n","Iteration 1770, loss = 0.07086272\n","Iteration 1771, loss = 0.07128260\n","Iteration 1772, loss = 0.07124052\n","Iteration 1773, loss = 0.07082505\n","Iteration 1774, loss = 0.07083265\n","Iteration 1775, loss = 0.07077091\n","Iteration 1776, loss = 0.07075325\n","Iteration 1777, loss = 0.07118049\n","Iteration 1778, loss = 0.07115619\n","Iteration 1779, loss = 0.07067296\n","Iteration 1780, loss = 0.07082431\n","Iteration 1781, loss = 0.07060198\n","Iteration 1782, loss = 0.07059633\n","Iteration 1783, loss = 0.07095812\n","Iteration 1784, loss = 0.07052489\n","Iteration 1785, loss = 0.07037206\n","Iteration 1786, loss = 0.07033076\n","Iteration 1787, loss = 0.07028847\n","Iteration 1788, loss = 0.07033925\n","Iteration 1789, loss = 0.07037977\n","Iteration 1790, loss = 0.07046493\n","Iteration 1791, loss = 0.07025684\n","Iteration 1792, loss = 0.07030239\n","Iteration 1793, loss = 0.07078117\n","Iteration 1794, loss = 0.07034690\n","Iteration 1795, loss = 0.07013093\n","Iteration 1796, loss = 0.07011046\n","Iteration 1797, loss = 0.07003854\n","Iteration 1798, loss = 0.07080284\n","Iteration 1799, loss = 0.07056120\n","Iteration 1800, loss = 0.07021271\n","Iteration 1801, loss = 0.07006016\n","Iteration 1802, loss = 0.06990615\n","Iteration 1803, loss = 0.07046460\n","Iteration 1804, loss = 0.07023094\n","Iteration 1805, loss = 0.06983104\n","Iteration 1806, loss = 0.07019310\n","Iteration 1807, loss = 0.06975768\n","Iteration 1808, loss = 0.06980838\n","Iteration 1809, loss = 0.06971496\n","Iteration 1810, loss = 0.06978900\n","Iteration 1811, loss = 0.06965644\n","Iteration 1812, loss = 0.06966405\n","Iteration 1813, loss = 0.06980306\n","Iteration 1814, loss = 0.06969235\n","Iteration 1815, loss = 0.06957219\n","Iteration 1816, loss = 0.06958981\n","Iteration 1817, loss = 0.06966071\n","Iteration 1818, loss = 0.06946567\n","Iteration 1819, loss = 0.06956256\n","Iteration 1820, loss = 0.06940772\n","Iteration 1821, loss = 0.06952789\n","Iteration 1822, loss = 0.06941931\n","Iteration 1823, loss = 0.06981044\n","Iteration 1824, loss = 0.06941422\n","Iteration 1825, loss = 0.06961436\n","Iteration 1826, loss = 0.06945392\n","Iteration 1827, loss = 0.06940766\n","Iteration 1828, loss = 0.06927236\n","Iteration 1829, loss = 0.06918630\n","Iteration 1830, loss = 0.06930730\n","Iteration 1831, loss = 0.06942221\n","Iteration 1832, loss = 0.06911100\n","Iteration 1833, loss = 0.06924828\n","Iteration 1834, loss = 0.06909598\n","Iteration 1835, loss = 0.06954858\n","Iteration 1836, loss = 0.06926541\n","Iteration 1837, loss = 0.06907634\n","Iteration 1838, loss = 0.06900738\n","Iteration 1839, loss = 0.06896085\n","Iteration 1840, loss = 0.06907764\n","Iteration 1841, loss = 0.06882638\n","Iteration 1842, loss = 0.06892021\n","Iteration 1843, loss = 0.06917138\n","Iteration 1844, loss = 0.06887279\n","Iteration 1845, loss = 0.06888605\n","Iteration 1846, loss = 0.06888942\n","Iteration 1847, loss = 0.06885260\n","Iteration 1848, loss = 0.06871287\n","Iteration 1849, loss = 0.06871888\n","Iteration 1850, loss = 0.06895827\n","Iteration 1851, loss = 0.06871435\n","Iteration 1852, loss = 0.06860388\n","Iteration 1853, loss = 0.06877422\n","Iteration 1854, loss = 0.06875070\n","Iteration 1855, loss = 0.06868686\n","Iteration 1856, loss = 0.06869335\n","Iteration 1857, loss = 0.06853319\n","Iteration 1858, loss = 0.06851871\n","Iteration 1859, loss = 0.06862445\n","Iteration 1860, loss = 0.06846010\n","Iteration 1861, loss = 0.06850153\n","Iteration 1862, loss = 0.06893407\n","Iteration 1863, loss = 0.06882620\n","Iteration 1864, loss = 0.06847045\n","Iteration 1865, loss = 0.06915891\n","Iteration 1866, loss = 0.06896187\n","Iteration 1867, loss = 0.06823270\n","Iteration 1868, loss = 0.06838605\n","Iteration 1869, loss = 0.06842074\n","Iteration 1870, loss = 0.06833280\n","Iteration 1871, loss = 0.06832567\n","Iteration 1872, loss = 0.06820059\n","Iteration 1873, loss = 0.06848629\n","Iteration 1874, loss = 0.06816420\n","Iteration 1875, loss = 0.06825088\n","Iteration 1876, loss = 0.06887925\n","Iteration 1877, loss = 0.06818949\n","Iteration 1878, loss = 0.06810968\n","Iteration 1879, loss = 0.06813164\n","Iteration 1880, loss = 0.06835202\n","Iteration 1881, loss = 0.06805637\n","Iteration 1882, loss = 0.06808863\n","Iteration 1883, loss = 0.06786849\n","Iteration 1884, loss = 0.06786834\n","Iteration 1885, loss = 0.06793101\n","Iteration 1886, loss = 0.06784090\n","Iteration 1887, loss = 0.06802808\n","Iteration 1888, loss = 0.06827648\n","Iteration 1889, loss = 0.06804411\n","Iteration 1890, loss = 0.06806099\n","Iteration 1891, loss = 0.06802983\n","Iteration 1892, loss = 0.06777098\n","Iteration 1893, loss = 0.06764999\n","Iteration 1894, loss = 0.06791639\n","Iteration 1895, loss = 0.06767344\n","Iteration 1896, loss = 0.06778640\n","Iteration 1897, loss = 0.06766008\n","Iteration 1898, loss = 0.06755475\n","Iteration 1899, loss = 0.06766688\n","Iteration 1900, loss = 0.06774482\n","Iteration 1901, loss = 0.06770364\n","Iteration 1902, loss = 0.06748265\n","Iteration 1903, loss = 0.06751060\n","Iteration 1904, loss = 0.06755179\n","Iteration 1905, loss = 0.06751777\n","Iteration 1906, loss = 0.06754618\n","Iteration 1907, loss = 0.06773226\n","Iteration 1908, loss = 0.06739379\n","Iteration 1909, loss = 0.06760353\n","Iteration 1910, loss = 0.06733790\n","Iteration 1911, loss = 0.06733701\n","Iteration 1912, loss = 0.06727174\n","Iteration 1913, loss = 0.06738984\n","Iteration 1914, loss = 0.06749483\n","Iteration 1915, loss = 0.06736762\n","Iteration 1916, loss = 0.06735554\n","Iteration 1917, loss = 0.06740271\n","Iteration 1918, loss = 0.06725980\n","Iteration 1919, loss = 0.06713540\n","Iteration 1920, loss = 0.06716865\n","Iteration 1921, loss = 0.06714453\n","Iteration 1922, loss = 0.06716465\n","Iteration 1923, loss = 0.06719989\n","Iteration 1924, loss = 0.06708788\n","Iteration 1925, loss = 0.06739221\n","Iteration 1926, loss = 0.06700872\n","Iteration 1927, loss = 0.06724300\n","Iteration 1928, loss = 0.06719290\n","Iteration 1929, loss = 0.06693571\n","Iteration 1930, loss = 0.06698106\n","Iteration 1931, loss = 0.06695226\n","Iteration 1932, loss = 0.06704213\n","Iteration 1933, loss = 0.06700268\n","Iteration 1934, loss = 0.06715115\n","Iteration 1935, loss = 0.06706012\n","Iteration 1936, loss = 0.06678604\n","Iteration 1937, loss = 0.06689014\n","Iteration 1938, loss = 0.06683075\n","Iteration 1939, loss = 0.06734825\n","Iteration 1940, loss = 0.06711151\n","Iteration 1941, loss = 0.06682122\n","Iteration 1942, loss = 0.06680308\n","Iteration 1943, loss = 0.06666386\n","Iteration 1944, loss = 0.06698612\n","Iteration 1945, loss = 0.06678723\n","Iteration 1946, loss = 0.06668962\n","Iteration 1947, loss = 0.06662542\n","Iteration 1948, loss = 0.06654271\n","Iteration 1949, loss = 0.06658151\n","Iteration 1950, loss = 0.06649687\n","Iteration 1951, loss = 0.06645785\n","Iteration 1952, loss = 0.06662926\n","Iteration 1953, loss = 0.06643144\n","Iteration 1954, loss = 0.06648308\n","Iteration 1955, loss = 0.06636126\n","Iteration 1956, loss = 0.06639743\n","Iteration 1957, loss = 0.06644626\n","Iteration 1958, loss = 0.06637379\n","Iteration 1959, loss = 0.06645647\n","Iteration 1960, loss = 0.06638114\n","Iteration 1961, loss = 0.06651174\n","Iteration 1962, loss = 0.06658393\n","Iteration 1963, loss = 0.06656662\n","Iteration 1964, loss = 0.06626528\n","Iteration 1965, loss = 0.06695741\n","Iteration 1966, loss = 0.06628045\n","Iteration 1967, loss = 0.06630988\n","Iteration 1968, loss = 0.06617038\n","Iteration 1969, loss = 0.06616724\n","Iteration 1970, loss = 0.06620124\n","Iteration 1971, loss = 0.06613377\n","Iteration 1972, loss = 0.06612396\n","Iteration 1973, loss = 0.06678221\n","Iteration 1974, loss = 0.06628890\n","Iteration 1975, loss = 0.06614748\n","Iteration 1976, loss = 0.06622447\n","Iteration 1977, loss = 0.06607521\n","Iteration 1978, loss = 0.06618500\n","Iteration 1979, loss = 0.06601321\n","Iteration 1980, loss = 0.06597762\n","Iteration 1981, loss = 0.06618633\n","Iteration 1982, loss = 0.06628914\n","Iteration 1983, loss = 0.06591167\n","Iteration 1984, loss = 0.06585771\n","Iteration 1985, loss = 0.06588369\n","Iteration 1986, loss = 0.06580707\n","Iteration 1987, loss = 0.06592489\n","Iteration 1988, loss = 0.06623886\n","Iteration 1989, loss = 0.06583866\n","Iteration 1990, loss = 0.06578948\n","Iteration 1991, loss = 0.06584040\n","Iteration 1992, loss = 0.06581990\n","Iteration 1993, loss = 0.06596052\n","Iteration 1994, loss = 0.06564055\n","Iteration 1995, loss = 0.06615587\n","Iteration 1996, loss = 0.06582124\n","Iteration 1997, loss = 0.06568638\n","Iteration 1998, loss = 0.06563890\n","Iteration 1999, loss = 0.06565364\n","Iteration 2000, loss = 0.06567124\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='logistic', batch_size=32, hidden_layer_sizes=(4, 4),\n","              max_iter=2000, tol=1e-05, verbose=True)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# weights\n","print(network.coefs_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FP_54DP9_Ipx","executionInfo":{"status":"ok","timestamp":1670732736846,"user_tz":-540,"elapsed":28,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"557c6d3a-491d-4cf1-ccbe-aa1d80638f13"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([[-0.71411823,  0.60188   , -0.19265534, -0.62626382],\n","       [-1.23504906,  1.11115396, -1.83665399, -1.00998014],\n","       [ 1.46153014, -1.30148699,  1.99631629,  1.04253212],\n","       [ 1.86143732, -1.73375453,  2.36434964,  2.0647603 ]]), array([[ 2.3887783 , -3.34477658, -2.3011233 ,  2.39283778],\n","       [-4.18588954,  3.64786096,  1.93770421, -1.74244125],\n","       [ 0.90316731, -0.38093454, -4.91245094,  4.07077164],\n","       [ 2.34729212, -2.44373243, -3.56759919,  3.01154995]]), array([[-2.24614638, -1.36902224,  3.05773827],\n","       [ 1.74306686,  2.55688595, -4.22914451],\n","       [ 4.20091592, -3.92591701, -2.51248976],\n","       [-4.23652386,  0.2910476 ,  1.30828023]])]\n"]}]},{"cell_type":"code","source":["# bias\n","print(network.intercepts_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m5vIr822_KTQ","executionInfo":{"status":"ok","timestamp":1670732736847,"user_tz":-540,"elapsed":13,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"aba219eb-03a3-4906-911c-8043bc9376cf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([-1.98299104,  2.10112834, -0.38703184, -1.43811801]), array([-1.73978087,  2.12368353,  1.22044228, -1.25867505]), array([-0.75129584,  0.13134765, -0.00147951])]\n"]}]},{"cell_type":"code","source":["# activation method\n","print(network.out_activation_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yj7jqdDV_V2L","executionInfo":{"status":"ok","timestamp":1670732736847,"user_tz":-540,"elapsed":12,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"7933e953-9fad-4e58-8aab-bcd9c40d432d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["softmax\n"]}]},{"cell_type":"markdown","metadata":{"id":"RTaTfbXNmUrn"},"source":["## Neural network (evaluation)"]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, confusion_matrix\n","predictions = network.predict(X_test)\n","print(accuracy_score(y_test, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYKwjZOv80Ls","executionInfo":{"status":"ok","timestamp":1670732736848,"user_tz":-540,"elapsed":12,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"dc1c6814-eae9-4d24-e83e-7d49a322d4a3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n"]}]},{"cell_type":"code","source":["cm = confusion_matrix(y_test, predictions)\n","print(cm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdZLCbrXaSQ-","executionInfo":{"status":"ok","timestamp":1670732737293,"user_tz":-540,"elapsed":456,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"b19acdfa-75db-4197-8adc-53f5deca02e9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[11  0  0]\n"," [ 0 13  0]\n"," [ 0  0  6]]\n"]}]},{"cell_type":"code","source":["!pip install yellowbrick --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPAL7kwAabs2","executionInfo":{"status":"ok","timestamp":1670732745003,"user_tz":-540,"elapsed":7712,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"2283beaa-0f4c-463f-955d-9a67ae908e71"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: yellowbrick in /usr/local/lib/python3.8/dist-packages (1.5)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from yellowbrick) (1.0.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from yellowbrick) (1.21.6)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from yellowbrick) (0.11.0)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from yellowbrick) (1.7.3)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from yellowbrick) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (3.1.0)\n"]}]},{"cell_type":"code","source":["from yellowbrick.classifier import ConfusionMatrix\n","confusion_matrix = ConfusionMatrix(network, classes = iris.target_names)\n","confusion_matrix.fit(X_train, y_train)\n","confusion_matrix.score(X_test, y_test)\n","confusion_matrix.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"hNGqoY1yaeeP","executionInfo":{"status":"ok","timestamp":1670732745841,"user_tz":-540,"elapsed":841,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"03511031-93d7-4058-998c-e9475fd321f6"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 576x396 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f4afca29220>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"Nkf_QRkpmZ7u"},"source":["## Neural network (classification)"]},{"cell_type":"code","source":["X_test_transposed = X_test[0].reshape(1, -1)\n","print(iris.target_names[network.predict(X_test_transposed)])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSX9I7MobGUR","executionInfo":{"status":"ok","timestamp":1670732745841,"user_tz":-540,"elapsed":4,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"a4d5e5ff-3fc3-4b28-ecbd-0ed9444be77a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['virginica']\n"]}]}]}